{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK+F5yeZwGzH/F3KeDscMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/haystack/blob/master/Utilizing_existing_FAQs_for_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizing existing FAQs for Question Answering\n",
        "\n",
        "While *extractive Question Answering* works on pure texts and is therefore more generalizable, there's also a common alternative that utilizes existing FAQ data.\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "- Very fast at inference time\n",
        "- Utilize existing FAQ data\n",
        "- Quite good control over answers\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "- Generalizability: We can only answer questions that are similar to existing ones in FAQ\n",
        "\n",
        "In some use cases, a combination of extractive QA and FAQ-style can also be an interesting option."
      ],
      "metadata": {
        "id": "-MI0KHYYvlxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEHhlWbdtp9Y",
        "outputId": "818f7f27-549d-49e6-e269-870011f321d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 17 18:00:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHVydjUdv99y",
        "outputId": "3b1671bd-81e3-41b8-e80b-1186fecb03a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farm-haystack[colab]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-8fxlyp_1/farm-haystack_d22c34feb6d6468897d1fcacd0d91900\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit 1399681c818c03326b6742c410aca97dc9c96b60\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting azure-ai-formrecognizer>=3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.4/228.4 kB 22.7 MB/s eta 0:00:00\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.64.1)\n",
            "Collecting elasticsearch<8,>=7.7\n",
            "  Downloading elasticsearch-7.17.7-py2.py3-none-any.whl (385 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.8/385.8 kB 39.7 MB/s eta 0:00:00\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15\n",
            "  Downloading rapidfuzz-2.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 78.8 MB/s eta 0:00:00\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 58.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 78.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.23.0)\n",
            "Requirement already satisfied: torch<1.13,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.12.1+cu113)\n",
            "Collecting huggingface-hub>=0.5.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.1/182.1 kB 21.8 MB/s eta 0:00:00\n",
            "Collecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 6.2 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.10.2)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.30.0-py3-none-any.whl (17.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 78.5 MB/s eta 0:00:00\n",
            "Collecting quantulum3\n",
            "  Downloading quantulum3-0.7.11-py3-none-any.whl (10.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 102.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.7.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.6.3)\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 97.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.3.3)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 6.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (0.3.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.13.0)\n",
            "Collecting posthog\n",
            "  Downloading posthog-2.2.0-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (9.0.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.0.2)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 8.9 MB/s eta 0:00:00\n",
            "Collecting grpcio==1.47.0\n",
            "  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 91.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio==1.47.0->farm-haystack[colab]) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 103.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.1.1)\n",
            "Collecting azure-core<2.0.0,>=1.23.0\n",
            "  Downloading azure_core-1.26.1-py3-none-any.whl (172 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.6/172.6 kB 23.1 MB/s eta 0:00:00\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 8.2 MB/s eta 0:00:00\n",
            "Collecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (2022.9.24)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0\n",
            "  Downloading jarowinkler-1.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.7/113.7 kB 12.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.13.1+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack[colab]) (3.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (5.10.0)\n",
            "Collecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 10.9 MB/s eta 0:00:00\n",
            "Collecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.5/147.5 kB 17.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (3.19.6)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
            "Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.4.43)\n",
            "Collecting prometheus-flask-exporter<1\n",
            "  Downloading prometheus_flask_exporter-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (7.1.2)\n",
            "Collecting alembic<2\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 kB 22.0 MB/s eta 0:00:00\n",
            "Collecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.3.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 10.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.1.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4.3)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.5.0)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (2022.6)\n",
            "Collecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.5/182.5 kB 23.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff<2.0.0,>=1.10.0\n",
            "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack[colab]) (4.9.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.2/125.2 kB 17.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 10.5 MB/s eta 0:00:00\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (0.8.10)\n",
            "Collecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 19.4 MB/s eta 0:00:00\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 8.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.11.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 8.7 MB/s eta 0:00:00\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 6.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2->farm-haystack[colab]) (3.0.9)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter<1->mlflow->farm-haystack[colab]) (0.15.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow->farm-haystack[colab]) (2.0.1)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack[colab]) (7.1.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask<3->mlflow->farm-haystack[colab]) (2.0.1)\n",
            "Building wheels for collected packages: sentence-transformers, farm-haystack, langdetect, python-docx, seqeval, tika, databricks-cli, docopt\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=8b23ee724be1e6fd3520d7cbe1351f0389212131a4cc1889e08b83a45e1ebe3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\n",
            "  Building wheel for farm-haystack (pyproject.toml): started\n",
            "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.12.0rc0-py3-none-any.whl size=588237 sha256=692d6ed9889934cdd0e10b30a5c2d1c93c0568ea87df444d97ca0664c5a932db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2lb5qa3v/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=2433fd8990b08aa41aecc642c9af55c5ec5cc15dc3138da2e771dade0a353378\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/b2/db/0c9b9eb7a44bf85ec0b42c06ee617d0a0de66840dc0b3248d1\n",
            "  Building wheel for python-docx (setup.py): started\n",
            "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=025b0a6984d400095da7b65f40aad262c5fb6bbc51e7487d18a633f8aebef0e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/43/ab/6dfe2e7103b24fb1148e95b265fd71d23d29bcbaa60a4a7ed9\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16182 sha256=a28bc568d06b5650206a5a53714055bdca7b44d6717c1f74a04ab1293ace0b28\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/a1/b7/0d3b008d0c77cd57332d724b92cf7650b4185b493dc785f00a\n",
            "  Building wheel for tika (setup.py): started\n",
            "  Building wheel for tika (setup.py): finished with status 'done'\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32892 sha256=ef010413a0e5bd8cd2f1a3dc31f888b56a9f9181bacbbc3388433e0b3d891930\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ca/3b/d3edbb6a53ef11ebc2ad6175a52f85432270a129f5e00d597c\n",
            "  Building wheel for databricks-cli (setup.py): started\n",
            "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139103 sha256=7be3af009f9fce287ab721bf1a7723da218324468563f02039316c673ee870d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/e9/d3/a2a128f8c9e17787e860355c1609403f6c8147d75adf2a26b2\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=4932985473d37c1cc6305edb5abb677bc868201b0d9d4e39d17fb3772af5c157\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/e0/34/704e15d0b49f199ac74a2757d018d428ad4cf119c078c80f16\n",
            "Successfully built sentence-transformers farm-haystack langdetect python-docx seqeval tika databricks-cli docopt\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, mmh3, docopt, azure-common, websocket-client, urllib3, smmap, querystring-parser, python-docx, pyjwt, num2words, langdetect, jarowinkler, isodate, gunicorn, grpcio, backoff, requests, rapidfuzz, quantulum3, Mako, gitdb, elasticsearch, tika, seqeval, prometheus-flask-exporter, posthog, huggingface-hub, gitpython, docker, databricks-cli, azure-core, alembic, transformers, msrest, mlflow, sentence-transformers, azure-ai-formrecognizer, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.50.0\n",
            "    Uninstalling grpcio-1.50.0:\n",
            "      Successfully uninstalled grpcio-1.50.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 azure-ai-formrecognizer-3.2.0 azure-common-1.1.28 azure-core-1.26.1 backoff-1.11.1 databricks-cli-0.17.3 docker-6.0.1 docopt-0.6.2 elasticsearch-7.17.7 farm-haystack-1.12.0rc0 gitdb-4.0.9 gitpython-3.1.29 grpcio-1.47.0 gunicorn-20.1.0 huggingface-hub-0.11.0 isodate-0.6.1 jarowinkler-1.2.3 langdetect-1.0.9 mlflow-1.30.0 mmh3-3.0.0 monotonic-1.6 msrest-0.7.1 num2words-0.5.12 posthog-2.2.0 prometheus-flask-exporter-0.21.0 pyjwt-2.6.0 python-docx-0.8.11 quantulum3-0.7.11 querystring-parser-1.2.4 rapidfuzz-2.7.0 requests-2.28.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 seqeval-1.2.2 smmap-5.0.0 tika-1.24 tokenizers-0.12.1 transformers-4.21.2 urllib3-1.26.12 websocket-client-1.4.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-8fxlyp_1/farm-haystack_d22c34feb6d6468897d1fcacd0d91900\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.47.0 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logging\n",
        "\n",
        "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
        "Example log message:\n",
        "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
        "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
      ],
      "metadata": {
        "id": "rGxRdctbwDqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "IaSQrJs9wEXT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start an Elasticsearch server\n",
        "You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source."
      ],
      "metadata": {
        "id": "CztTD1urwQq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended: Start Elasticsearch using Docker via the Haystack utility function\n",
        "from haystack.utils import launch_es\n",
        "\n",
        "launch_es()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRi4eIOfwRUq",
        "outputId": "1ddd4a08-8eb8-4534-d64f-f047f25c6fac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.utils.doc_store:Tried to start Elasticsearch through Docker but this failed. It is likely that there is already an existing Elasticsearch instance running. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start an Elasticsearch server in Colab\n",
        "\n",
        "If Docker is not readily available in your environment (e.g. in Colab notebooks), then you can manually download and execute Elasticsearch from source."
      ],
      "metadata": {
        "id": "Fkh6A3H4wjZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch -d"
      ],
      "metadata": {
        "id": "Hevqu5wdwk_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Init the DocumentStore\n",
        "In contrast to Tutorial 1 (Build your first QA system), we:\n",
        "\n",
        "* specify the name of our `text_field` in Elasticsearch that we want to return as an answer\n",
        "* specify the name of our `embedding_field` in Elasticsearch where we'll store the embedding of our question and that is used later for calculating our similarity to the incoming user question\n",
        "* set `excluded_meta_data=[\"question_emb\"]` so that we don't return the huge embedding vectors in our search results"
      ],
      "metadata": {
        "id": "FRjEGUg8xYYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "# Wait 30 seconds only to be sure Elasticsearch is ready before continuing\n",
        "time.sleep(30)\n",
        "\n",
        "# Get the host where Elasticsearch is running, default to localhost\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(\n",
        "    host=host,\n",
        "    username=\"\",\n",
        "    password=\"\",\n",
        "    index=\"document\",\n",
        "    embedding_field=\"question_emb\",\n",
        "    embedding_dim=384,\n",
        "    excluded_meta_data=[\"question_emb\"],\n",
        "    similarity=\"cosine\",\n",
        ")"
      ],
      "metadata": {
        "id": "-au3dc49xqvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Retriever using embeddings\n",
        "Instead of retrieving via Elasticsearch's plain BM25, we want to use vector similarity of the questions (user question vs. FAQ ones).\n",
        "We can use the `EmbeddingRetriever` for this purpose and specify a model that we use for the embeddings."
      ],
      "metadata": {
        "id": "EpEam-T6y-1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    use_gpu=True,\n",
        "    scale_score=False,\n",
        ")"
      ],
      "metadata": {
        "id": "NLLhjTHGzAYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare & Index FAQ data\n",
        "We create a pandas dataframe containing some FAQ data (i.e curated pairs of question + answer) and index those in elasticsearch.\n",
        "Here: We download some question-answer pairs related to COVID-19"
      ],
      "metadata": {
        "id": "m9IFrtsLz5kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from haystack.utils import fetch_archive_from_http\n",
        "\n",
        "\n",
        "# Download\n",
        "doc_dir = \"data/tutorial4\"\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/small_faq_covid.csv.zip\"\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "# Get dataframe with columns \"question\", \"answer\" and some custom metadata\n",
        "df = pd.read_csv(f\"{doc_dir}/small_faq_covid.csv\")\n",
        "# Minimal cleaning\n",
        "df.fillna(value=\"\", inplace=True)\n",
        "df[\"question\"] = df[\"question\"].apply(lambda x: x.strip())\n",
        "print(df.head())\n",
        "\n",
        "# Get embeddings for our questions from the FAQs\n",
        "questions = list(df[\"question\"].values)\n",
        "df[\"question_emb\"] = retriever.embed_queries(queries=questions).tolist()\n",
        "df = df.rename(columns={\"question\": \"content\"})\n",
        "\n",
        "# Convert Dataframe to list of dicts and index them in our DocumentStore\n",
        "docs_to_index = df.to_dict(orient=\"records\")\n",
        "document_store.write_documents(docs_to_index)"
      ],
      "metadata": {
        "id": "4g2TFGQ_z0xK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}